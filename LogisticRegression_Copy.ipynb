{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Implement Simple Linear Regression From Scratch With Python\n",
    "https://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            #if not row:\n",
    "            #    continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "dataset = load_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove column header and convert all the columns to float. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "col_names = dataset[0]\n",
    "dataset.pop(0)\n",
    "\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 17.0],\n",
       " [0.0, 199.0],\n",
       " [0.0, 122.0],\n",
       " [0.0, 99.0],\n",
       " [0.0, 846.0],\n",
       " [0.0, 67.1],\n",
       " [0.078, 2.42],\n",
       " [21.0, 81.0],\n",
       " [0.0, 1.0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        min_value = min(col_values)\n",
    "        max_value = max(col_values)\n",
    "        minmax.append([min_value,max_value])\n",
    "    return minmax\n",
    "\n",
    "minmax_data = dataset_minmax(dataset)\n",
    "minmax_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of each feature differs, so min-max normalization is used. All the values will fall into range (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i]-minmax[i][0]) / (minmax[i][1]-minmax[i][0])\n",
    "\n",
    "normalize_dataset(dataset, minmax_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def predict(row, coeff):\n",
    "    yhat = coeff[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coeff[i+1] * row[i]\n",
    "    prediction = 1.0/(1.0 + exp(-yhat))\n",
    "    return prediction\n",
    "\n",
    "def coeff_sgd(train, learning_rate, epoch):\n",
    "    coeff = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch_value in range(epoch):\n",
    "        for row in train:\n",
    "            yhat = predict(row, coeff)\n",
    "            error = row[-1] - yhat\n",
    "            coeff[0] = coeff[0] + learning_rate * error * yhat * (1.0 - yhat)\n",
    "            for i in range(len(row)-1):\n",
    "                coeff[i+1] = coeff[i+1] + learning_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "    return coeff\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(train, test, learning_rate, epoch):\n",
    "    predictions = list()\n",
    "    coeff = coeff_sgd(train, learning_rate, epoch)\n",
    "    for row in test:\n",
    "        yhat = round(predict(row, coeff))\n",
    "        predictions.append(yhat)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    true_neg = 0\n",
    "    true_pos = 0\n",
    "    false_neg = 0\n",
    "    false_pos = 0\n",
    "\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "            if actual[i]==0:\n",
    "                true_neg += 1\n",
    "            else:\n",
    "                true_pos +=1\n",
    "        else:\n",
    "            if actual[i]==0:\n",
    "                false_pos +=1\n",
    "            else:\n",
    "                false_neg +=1\n",
    "    confusion_matrix = [true_pos, false_neg, false_pos, true_neg]        \n",
    "    percentage = correct / float(len(actual)) * 100.0\n",
    "    return [confusion_matrix,percentage]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fold size = number of training data in each set.\n",
    "k-1 folds are used as training data.\n",
    "cross validation is used to measure prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "from random import seed\n",
    "\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset)/n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index)) #dataset_copy becomes the testing dataset (the remaining data)\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "def eval_algthm(dataset, algthm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    accuracy_list = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algthm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        confusion_matrix, accuracy = accuracy_metric(actual, predicted)\n",
    "        accuracy_list.append(accuracy)\n",
    "    return [confusion_matrix, accuracy_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing different values for k-fold, learning rate and epoch. Average accuracy is calculated w.r.t. each fold. k-fold value:4, learning rate: 0.2 and epoch value:10 seems to have the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "#n_folds = 5\n",
    "#learning_rate = 0.1\n",
    "#epoch = 100\n",
    "def score_df(data):\n",
    "    scores_list = list()\n",
    "    for i in range(3,10):\n",
    "        n_folds = i\n",
    "        for j in range(5, 35, 5):\n",
    "            learning_rate = j/100.0\n",
    "            for k in [50,100]:\n",
    "                epoch = k\n",
    "                confusion_matrix, scores = eval_algthm(data, logistic_regression, n_folds, learning_rate, epoch)\n",
    "                average_score = sum(scores)/float(len(scores))\n",
    "                scores_list.append([i,learning_rate, epoch, average_score, confusion_matrix])\n",
    "    return scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    k-fold  learning rate  epoch  average accuracy %   confusion matrix\n",
      "0        3           0.05     50           75.781250  [41, 41, 19, 155]\n",
      "1        3           0.05    100           76.171875  [59, 35, 19, 143]\n",
      "2        3           0.10     50           76.562500  [51, 40, 20, 145]\n",
      "3        3           0.10    100           76.822917  [50, 44, 20, 142]\n",
      "4        3           0.15     50           76.822917   [54, 45, 9, 148]\n",
      "5        3           0.15    100           76.562500  [48, 44, 13, 151]\n",
      "6        3           0.20     50           76.302083  [49, 39, 29, 139]\n",
      "7        3           0.20    100           77.083333  [48, 42, 20, 146]\n",
      "8        3           0.25     50           76.432292  [52, 46, 20, 138]\n",
      "9        3           0.25    100           77.473958  [57, 32, 15, 152]\n",
      "10       3           0.30     50           77.604167  [43, 38, 17, 158]\n",
      "11       3           0.30    100           76.692708  [69, 22, 28, 137]\n",
      "12       4           0.05     50           75.781250  [24, 31, 11, 126]\n",
      "13       4           0.05    100           76.692708  [39, 26, 15, 112]\n",
      "14       4           0.10     50           77.083333   [33, 29, 8, 122]\n",
      "15       4           0.10    100           76.692708  [48, 24, 14, 106]\n",
      "16       4           0.15     50           77.083333  [36, 24, 13, 119]\n",
      "17       4           0.15    100           76.302083   [49, 19, 26, 98]\n",
      "18       4           0.20     50           75.390625  [41, 27, 11, 113]\n",
      "19       4           0.20    100           77.994792  [33, 27, 12, 120]\n",
      "20       4           0.25     50           76.041667  [35, 35, 14, 108]\n",
      "21       4           0.25    100           77.213542  [41, 28, 15, 108]\n",
      "22       4           0.30     50           76.692708  [49, 21, 16, 106]\n",
      "23       4           0.30    100           76.302083   [40, 33, 22, 97]\n",
      "24       5           0.05     50           76.339869   [24, 26, 12, 91]\n",
      "25       5           0.05    100           76.601307   [28, 26, 13, 86]\n",
      "26       5           0.10     50           76.732026   [32, 25, 11, 85]\n",
      "27       5           0.10    100           76.862745   [27, 29, 12, 85]\n",
      "28       5           0.15     50           76.470588   [28, 25, 13, 87]\n",
      "29       5           0.15    100           77.647059   [37, 20, 15, 81]\n",
      "..     ...            ...    ...                 ...                ...\n",
      "54       7           0.20     50           77.719528    [20, 14, 9, 66]\n",
      "55       7           0.20    100           77.195282   [14, 14, 11, 70]\n",
      "56       7           0.25     50           77.064220    [23, 15, 7, 64]\n",
      "57       7           0.25    100           76.671035   [28, 12, 13, 56]\n",
      "58       7           0.30     50           75.622543    [24, 16, 9, 60]\n",
      "59       7           0.30    100           77.064220    [26, 17, 6, 60]\n",
      "60       8           0.05     50           76.302083     [22, 8, 7, 59]\n",
      "61       8           0.05    100           76.692708    [20, 14, 8, 54]\n",
      "62       8           0.10     50           76.171875    [27, 13, 6, 50]\n",
      "63       8           0.10    100           77.343750    [20, 9, 13, 54]\n",
      "64       8           0.15     50           77.083333    [22, 15, 6, 53]\n",
      "65       8           0.15    100           77.473958    [16, 10, 6, 64]\n",
      "66       8           0.20     50           77.604167    [13, 19, 5, 59]\n",
      "67       8           0.20    100           76.692708    [19, 17, 6, 54]\n",
      "68       8           0.25     50           76.302083    [14, 13, 8, 61]\n",
      "69       8           0.25    100           77.213542   [16, 13, 12, 55]\n",
      "70       8           0.30     50           75.130208   [19, 20, 12, 45]\n",
      "71       8           0.30    100           76.953125   [26, 16, 11, 43]\n",
      "72       9           0.05     50           76.339869    [12, 13, 6, 54]\n",
      "73       9           0.05    100           76.993464    [11, 15, 4, 55]\n",
      "74       9           0.10     50           76.601307    [18, 10, 8, 49]\n",
      "75       9           0.10    100           76.993464    [20, 17, 1, 47]\n",
      "76       9           0.15     50           75.686275    [16, 14, 7, 48]\n",
      "77       9           0.15    100           76.732026    [18, 14, 7, 46]\n",
      "78       9           0.20     50           77.777778     [18, 7, 6, 54]\n",
      "79       9           0.20    100           76.470588    [19, 10, 6, 50]\n",
      "80       9           0.25     50           76.209150    [15, 18, 4, 48]\n",
      "81       9           0.25    100           77.777778    [15, 12, 6, 52]\n",
      "82       9           0.30     50           76.339869    [19, 17, 5, 44]\n",
      "83       9           0.30    100           76.993464    [17, 15, 9, 44]\n",
      "\n",
      "[84 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#print(scores)\n",
    "#print(sum(scores)/float(len(scores)))\n",
    "import pandas as pd\n",
    "\n",
    "scores_list = score_df(dataset)\n",
    "results = pd.DataFrame(scores_list, columns = ('k-fold', 'learning rate', 'epoch', 'average accuracy %', 'confusion matrix'))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_accuracy(results):\n",
    "    max_accuracy = max(results.iloc[:,-2])\n",
    "    for i in range(len(results)):\n",
    "        if results.iloc[i][-2] == max_accuracy:\n",
    "            results_list = list()\n",
    "            confusion_matrix = results.iloc[i][-1]\n",
    "            for j in range(len(results.iloc[i])-2):\n",
    "                results_list.append(results.iloc[i][j])\n",
    "    print('k-fold, learning rate, epoch for max. average accuracy: ', results_list)\n",
    "    print('confusion matrix: ', confusion_matrix)\n",
    "    print('Average accuracy obtained: %0.3f' % max_accuracy,'%')\n",
    "    return results_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold, learning rate, epoch for max. average accuracy:  [4, 0.2, 100]\n",
      "confusion matrix:  [33, 27, 12, 120]\n",
      "Average accuracy obtained: 77.995 %\n",
      "Score in diff. no. of epoch:  [65.10416666666666, 76.30208333333334, 76.953125, 77.08333333333334, 77.99479166666666, 75.91145833333334, 75.390625, 76.82291666666666, 76.30208333333333, 76.171875, 76.5625]\n"
     ]
    }
   ],
   "source": [
    "results_list = find_max_accuracy(results)\n",
    "n_folds = results_list[0]\n",
    "learning_rate = results_list[1]\n",
    "max_epoch = results_list[2]\n",
    "epoch_score = list()\n",
    "for epoch in range(0, max_epoch+10,10):\n",
    "    confusion_matrix, scores = eval_algthm(dataset, logistic_regression, n_folds, learning_rate, epoch)\n",
    "    scores_avg =  sum(scores)/float(len(scores))\n",
    "    epoch_score.append(scores_avg)\n",
    "\n",
    "print('Score in diff. no. of epoch: ', epoch_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<matplotlib.lines.Line2D object at 0x000002CBE7F26CF8>]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_axis = [epoch for epoch in range(0, max_epoch+10, 10)]\n",
    "accuracy_plt = plt.plot(x_axis,epoch_score)\n",
    "#plt.xlim(xmax = max_epoch, xmin = 0)\n",
    "plt.ylim(ymax = 100, ymin = 50)\n",
    "plt.title('Average accuracy reached in every 10 epoch')\n",
    "print(accuracy_plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a different dataset to perform the logistic regression model with the defined functions above.\n",
    "\n",
    "The dataset 'voice.csv' is used. The labels are indicating whether the gender of the voice is 'male' or 'female'. The first step is to remove the column header and change the labels to 1 or 0, with 1 representing 'male' and 0 representing 'female'.\n",
    "\n",
    "Dataset can be found on: https://www.kaggle.com/primaryobjects/voicegender/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = load_csv('voice.csv')\n",
    "col_names = dataset_2[0]\n",
    "dataset_2.pop(0)\n",
    "for row in dataset_2:\n",
    "    if row[-1] =='male':\n",
    "        row[-1]=1\n",
    "    else:\n",
    "        row[-1]=0\n",
    "\n",
    "for i in range(len(dataset_2[0])-1):\n",
    "    str_column_to_float(dataset_2, i)\n",
    "\n",
    "minmax_data_2 = dataset_minmax(dataset_2)\n",
    "\n",
    "normalize_dataset(dataset_2, minmax_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    k-fold  learning rate  epoch  average accuracy %  confusion matrix\n",
      "79       9           0.20    100           97.474747  [168, 3, 5, 176]\n",
      "80       9           0.25     50           97.380051  [164, 6, 1, 181]\n",
      "81       9           0.25    100           97.032828  [160, 7, 4, 181]\n",
      "82       9           0.30     50           97.443182  [175, 6, 3, 168]\n",
      "83       9           0.30    100           97.537879  [170, 3, 4, 175]\n"
     ]
    }
   ],
   "source": [
    "scores_list_2 = score_df(dataset_2)\n",
    "results_2 = pd.DataFrame(scores_list_2, columns = ('k-fold', 'learning rate', 'epoch', 'average accuracy %', 'confusion matrix'))\n",
    "print(results_2.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold, learning rate, epoch for max. average accuracy:  [5, 0.3, 100]\n",
      "confusion matrix:  [320, 17, 4, 292]\n",
      "Average accuracy obtained: 97.662 %\n"
     ]
    }
   ],
   "source": [
    "results_list_2 = find_max_accuracy(results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
